{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12844889,"sourceType":"datasetVersion","datasetId":8124093}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"223a1a21","cell_type":"markdown","source":"# U-Net++ for Earthquake Damage Assessment\n### Configuration Matching SegFormer and BEiT\nThis notebook implements U-Net++ with the exact same hyperparameters as SegFormer for fair comparison.","metadata":{}},{"id":"ff0c737d","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"14905389","cell_type":"markdown","source":"## U-Net++ Architecture\nImplementing nested U-Net with dense skip connections and deep supervision.","metadata":{}},{"id":"813296c7","cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, dropout_rate=0.15):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(dropout_rate),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(dropout_rate)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass UNetPP(nn.Module):\n    def __init__(self, in_ch=6, out_ch=1, filters=(32,64,128,256,512), dropout_rates=None):\n        super().__init__()\n        \n        if dropout_rates is None:\n            # Consistent dropout matching SegFormer (0.15)\n            dropout_rates = {\n                'encoder': [0.0, 0.0, 0.0, 0.1, 0.15],\n                'decoder': [0.15, 0.15, 0.15, 0.15],\n                'skip': 0.15,\n                'final': 0.15\n            }\n        \n        self.dropout_rates = dropout_rates\n        f0, f1, f2, f3, f4 = filters\n        \n        # Encoder path (downsampling)\n        self.conv0_0 = ConvBlock(in_ch, f0, dropout_rates['encoder'][0])\n        self.conv1_0 = ConvBlock(f0, f1, dropout_rates['encoder'][1])\n        self.conv2_0 = ConvBlock(f1, f2, dropout_rates['encoder'][2])\n        self.conv3_0 = ConvBlock(f2, f3, dropout_rates['encoder'][3])\n        self.conv4_0 = ConvBlock(f3, f4, dropout_rates['encoder'][4])\n        \n        self.pool = nn.MaxPool2d(2)\n        \n        # Nested decoder path\n        self.conv0_1 = ConvBlock(f0+f1, f0, dropout_rates['decoder'][0])\n        self.conv1_1 = ConvBlock(f1+f2, f1, dropout_rates['decoder'][1])\n        self.conv2_1 = ConvBlock(f2+f3, f2, dropout_rates['decoder'][2])\n        self.conv3_1 = ConvBlock(f3+f4, f3, dropout_rates['decoder'][3])\n        \n        self.conv0_2 = ConvBlock(f0*2+f1, f0, dropout_rates['decoder'][0])\n        self.conv1_2 = ConvBlock(f1*2+f2, f1, dropout_rates['decoder'][1])\n        self.conv2_2 = ConvBlock(f2*2+f3, f2, dropout_rates['decoder'][2])\n        \n        self.conv0_3 = ConvBlock(f0*3+f1, f0, dropout_rates['decoder'][0])\n        self.conv1_3 = ConvBlock(f1*3+f2, f1, dropout_rates['decoder'][1])\n        \n        self.conv0_4 = ConvBlock(f0*4+f1, f0, dropout_rates['decoder'][0])\n        \n        # Upsampling\n        self.up1 = nn.ConvTranspose2d(f1, f1, 2, stride=2)\n        self.up2 = nn.ConvTranspose2d(f2, f2, 2, stride=2)\n        self.up3 = nn.ConvTranspose2d(f3, f3, 2, stride=2)\n        self.up4 = nn.ConvTranspose2d(f4, f4, 2, stride=2)\n        \n        # Final output\n        self.final = nn.Conv2d(f0, out_ch, 1)\n    \n    def forward(self, x):\n        # Encoder\n        x0_0 = self.conv0_0(x)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x4_0 = self.conv4_0(self.pool(x3_0))\n        \n        # Nested skip connections - Level 1\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up1(x1_0)], 1))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up2(x2_0)], 1))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up3(x3_0)], 1))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up4(x4_0)], 1))\n        \n        # Nested skip connections - Level 2\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up1(x1_1)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up2(x2_1)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up3(x3_1)], 1))\n        \n        # Nested skip connections - Level 3\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up1(x1_2)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up2(x2_2)], 1))\n        \n        # Nested skip connections - Level 4\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up1(x1_3)], 1))\n        \n        output = self.final(x0_4)\n        return output","metadata":{},"outputs":[],"execution_count":null},{"id":"04199931","cell_type":"markdown","source":"## Dataset and Data Loading","metadata":{}},{"id":"e577cd38","cell_type":"code","source":"class EarthquakeDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, img_size=(256, 256)):\n        self.root_dir = root_dir\n        self.split = split\n        self.transform = transform\n        self.img_size = img_size\n        \n        if split == 'train':\n            self.pre_dir = os.path.join(root_dir, 'train', 'A_train_aug')\n            self.post_dir = os.path.join(root_dir, 'train', 'B_train_aug')\n            self.label_dir = os.path.join(root_dir, 'train', 'label_train_aug')\n        elif split == 'val':\n            self.pre_dir = os.path.join(root_dir, 'val', 'A_val')\n            self.post_dir = os.path.join(root_dir, 'val', 'B_val')\n            self.label_dir = os.path.join(root_dir, 'val', 'label_val')\n        else:\n            self.pre_dir = os.path.join(root_dir, 'test', 'A_test')\n            self.post_dir = os.path.join(root_dir, 'test', 'B_test')\n            self.label_dir = os.path.join(root_dir, 'test', 'label_test')\n        \n        self.image_files = sorted([f for f in os.listdir(self.pre_dir) if f.endswith('.png')])\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        \n        pre_img = Image.open(os.path.join(self.pre_dir, img_name)).convert('RGB')\n        post_img = Image.open(os.path.join(self.post_dir, img_name)).convert('RGB')\n        label = Image.open(os.path.join(self.label_dir, img_name)).convert('L')\n        \n        pre_img = pre_img.resize(self.img_size)\n        post_img = post_img.resize(self.img_size)\n        label = label.resize(self.img_size)\n        \n        if self.transform:\n            pre_img = self.transform(pre_img)\n            post_img = self.transform(post_img)\n        else:\n            pre_img = transforms.ToTensor()(pre_img)\n            post_img = transforms.ToTensor()(post_img)\n        \n        label = transforms.ToTensor()(label)\n        label = (label > 0.5).float()\n        \n        combined_img = torch.cat([pre_img, post_img], dim=0)\n        \n        return combined_img, label","metadata":{},"outputs":[],"execution_count":null},{"id":"8e3bf061","cell_type":"markdown","source":"## Loss Functions and Metrics","metadata":{}},{"id":"8005e6c5","cell_type":"code","source":"def dice_coef(pred, target, smooth=1e-6):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    dice = (2. * intersection + smooth) / (denom + smooth)\n    return dice.mean()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        dice = dice_coef(pred, target, smooth=self.smooth)\n        return 1.0 - dice\n\ndef combined_loss(logits, mask, bce_weight=0.6, dice_weight=0.4):\n    bce_loss = nn.BCEWithLogitsLoss()\n    bce = bce_loss(logits, mask)\n    probs = torch.sigmoid(logits)\n    dice = DiceLoss()(probs, mask)\n    return bce_weight * bce + dice_weight * dice\n\n@torch.no_grad()\ndef compute_metrics_batch(logits, masks, thresh=0.5):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= thresh).float()\n    preds_flat = preds.view(-1).cpu().numpy()\n    masks_flat = masks.view(-1).cpu().numpy()\n    \n    unique_preds = np.unique(preds_flat)\n    unique_masks = np.unique(masks_flat)\n    \n    if len(unique_preds) == 1 and len(unique_masks) == 1:\n        if unique_preds[0] == unique_masks[0]:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = len(preds_flat), 0, 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, 0, len(preds_flat)\n        else:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = 0, len(preds_flat), 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, len(preds_flat), 0\n    else:\n        cm = confusion_matrix(masks_flat, preds_flat, labels=[0,1])\n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n        else:\n            if cm.shape == (1, 1):\n                if unique_masks[0] == 0:\n                    tn, fp, fn, tp = cm[0,0], 0, 0, 0\n                else:\n                    tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n            else:\n                tn, fp, fn, tp = 0, 0, 0, 0\n    \n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    iou_background = tn / (tn + fp + fn + eps)\n    iou_damage = tp / (tp + fp + fn + eps)\n    miou = (iou_background + iou_damage) / 2\n    \n    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n            \"iou\": float(iou), \"dice\": float(dice), \"precision\": float(precision),\n            \"recall\": float(recall), \"f1\": float(f1), \"acc\": float(acc), \"miou\": float(miou)}","metadata":{},"outputs":[],"execution_count":null},{"id":"76380461","cell_type":"markdown","source":"## Training and Validation Functions","metadata":{}},{"id":"2e5283d5","cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=10, min_delta=1e-4, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\ndef train_one_epoch(model, loader, optimizer):\n    model.train()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    \n    for imgs, masks in tqdm(loader, desc=\"Train batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n        \n        mets = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += mets[k]\n    \n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    train_acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    return epoch_loss, train_acc\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    \n    for imgs, masks in tqdm(loader, desc=\"Val batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        running_loss += loss.item() * imgs.size(0)\n        mets = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += mets[k]\n    \n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    iou_background = tn / (tn + fp + fn + eps)\n    iou_damage = tp / (tp + fp + fn + eps)\n    miou = (iou_background + iou_damage) / 2\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    metrics = {\"loss\": epoch_loss, \"iou\": iou, \"dice\": dice, \"precision\": precision,\n               \"recall\": recall, \"f1\": f1, \"acc\": acc, \"miou\": miou,\n               \"confusion\": np.array([[tn, fp], [fn, tp]])}\n    return metrics\n\ndef train_model(model, train_loader, val_loader, num_epochs=200, learning_rate=2e-4, patience=10):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        if p.ndim == 1 or name.endswith(\".bias\"):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    optimizer = optim.AdamW(\n        [\n            {\"params\": decay, \"weight_decay\": 1e-3},\n            {\"params\": no_decay, \"weight_decay\": 0.0},\n        ],\n        lr=learning_rate,\n    )\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.7, patience=7, verbose=True, min_lr=1e-6\n    )\n    \n    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n    \n    train_losses = []\n    val_losses = []\n    val_ious = []\n    val_dices = []\n    train_accs = []\n    val_accs = []\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n        \n        val_metrics = validate(model, val_loader)\n        \n        scheduler.step(val_metrics['loss'])\n        \n        print(f\"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_metrics['loss']:.6f} | Val Acc: {val_metrics['acc']:.4f} | IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | F1: {val_metrics['f1']:.4f}\")\n        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_metrics['loss'])\n        val_ious.append(val_metrics['iou'])\n        val_dices.append(val_metrics['dice'])\n        train_accs.append(train_acc)\n        val_accs.append(val_metrics['acc'])\n        \n        if val_metrics['loss'] < best_val_loss - 1e-4:\n            best_val_loss = val_metrics['loss']\n            torch.save(model.state_dict(), 'best_unetpp_earthquake.pth')\n            print(f\"Saved best model with validation loss: {best_val_loss:.6f}\")\n        \n        if early_stopping(val_metrics['loss'], model):\n            print(f'Early stopping triggered after {epoch+1} epochs')\n            break\n    \n    return train_losses, val_losses, val_ious, val_dices, train_accs, val_accs","metadata":{},"outputs":[],"execution_count":null},{"id":"9d76680d","cell_type":"markdown","source":"## Training Configuration and Execution\n### Using SegFormer configurations: lr=2e-4, weight_decay=1e-3, dropout=0.15, threshold=0.5","metadata":{}},{"id":"4d753e46","cell_type":"code","source":"def main():\n    # Hyperparameters matching SegFormer\n    BATCH_SIZE = 6\n    LEARNING_RATE = 2e-4\n    NUM_EPOCHS = 200\n    PATIENCE = 10\n    IMG_SIZE = (256, 256)\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    root_dir = '/kaggle/input/finaldatasetnew/earthquakedatasetnew'\n    \n    train_dataset = EarthquakeDataset(root_dir, split='train', transform=transform, img_size=IMG_SIZE)\n    val_dataset = EarthquakeDataset(root_dir, split='val', transform=transform, img_size=IMG_SIZE)\n    test_dataset = EarthquakeDataset(root_dir, split='test', transform=transform, img_size=IMG_SIZE)\n    \n    train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n    \n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    print(f\"Test samples: {len(test_dataset)}\")\n    \n    # Consistent dropout configuration matching SegFormer\n    dropout_config = {\n        'encoder': [0.0, 0.0, 0.0, 0.1, 0.15],\n        'decoder': [0.15, 0.15, 0.15, 0.15],\n        'skip': 0.15,\n        'final': 0.15\n    }\n    \n    model = UNetPP(in_ch=6, out_ch=1, filters=(32,64,128,256,512), \n                   dropout_rates=dropout_config).to(device)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")\n    \n    print(\"\\nStarting U-Net++ training with SegFormer configuration...\")\n    print(f\"Learning Rate: {LEARNING_RATE}\")\n    print(f\"Weight Decay: 1e-3\")\n    print(f\"Dropout: Consistent 0.15\")\n    print(f\"Threshold: 0.5\")\n    print(f\"Scheduler: ReduceLROnPlateau (factor=0.7, patience=7)\\n\")\n    \n    train_losses, val_losses, val_ious, val_dices, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE, PATIENCE\n    )\n    \n    return model, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader\n\nmodel, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader = main()","metadata":{},"outputs":[],"execution_count":null},{"id":"577ac228","cell_type":"markdown","source":"## Visualization of Training Progress","metadata":{}},{"id":"e946b3c8","cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\nax1.plot(train_losses, label='Training Loss')\nax1.plot(val_losses, label='Validation Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_ylim(0, 1)\nax1.legend()\nax1.set_title('U-Net++ Training and Validation Loss')\n\nax2.plot(train_accs, label='Training Accuracy', color='blue')\nax2.plot(val_accs, label='Validation Accuracy', color='red')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.set_ylim(0, 1)\nax2.legend()\nax2.set_title('U-Net++ Training and Validation Accuracy')\n\nax3.plot(val_ious, label='Validation IoU', color='green')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('IoU')\nax3.set_ylim(0, 1)\nax3.legend()\nax3.set_title('U-Net++ Validation IoU')\n\nax4.plot(val_dices, label='Validation Dice', color='orange')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Dice Coefficient')\nax4.set_ylim(0, 1)\nax4.legend()\nax4.set_title('U-Net++ Validation Dice Coefficient')\n\nplt.tight_layout()\nplt.savefig('unetpp_training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"428c605c","cell_type":"markdown","source":"## Test Evaluation","metadata":{}},{"id":"310569b5","cell_type":"code","source":"print(\"Loading best U-Net++ model for test evaluation...\")\nmodel.load_state_dict(torch.load('best_unetpp_earthquake.pth', map_location=device))\nmodel.to(device)\n\nprint(\"\\nEvaluating U-Net++ on test set...\")\ntest_metrics = validate(model, test_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"U-NET++ TEST EVALUATION METRICS\")\nprint(\"Configuration: Matching SegFormer (lr=2e-4, dropout=0.15, threshold=0.5)\")\nprint(\"=\"*50)\nprint(f\"Test set processed with batch_size=1\")\nprint(f\"Loss:            {test_metrics['loss']:.6f}\")\nprint(f\"IoU:             {test_metrics['iou']:.4f}\")\nprint(f\"mIoU:            {test_metrics['miou']:.4f}\")\nprint(f\"Dice Coefficient: {test_metrics['dice']:.4f}\")\nprint(f\"Accuracy:        {test_metrics['acc']:.4f}\")\nprint(f\"Precision:       {test_metrics['precision']:.4f}\")\nprint(f\"Recall:          {test_metrics['recall']:.4f}\")\nprint(f\"F1-Score:        {test_metrics['f1']:.4f}\")\nprint(\"Confusion matrix (pixel-level):\")\nprint(test_metrics[\"confusion\"])\nprint(\"=\"*50)\n\nplt.figure(figsize=(8, 6))\ncm = test_metrics[\"confusion\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Predicted No Damage', 'Predicted Damage'],\n            yticklabels=['Actual No Damage', 'Actual Damage'])\nplt.title('U-Net++ Test Set Confusion Matrix (Pixel-level)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.text(0.5, -0.1, f'TN: {cm[0,0]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(1.5, -0.1, f'FP: {cm[0,1]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(0.5, -0.15, f'FN: {cm[1,0]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(1.5, -0.15, f'TP: {cm[1,1]:,}', ha='center', transform=plt.gca().transAxes)\n\nplt.tight_layout()\nplt.savefig('unetpp_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"U-Net++ training and evaluation completed!\")\nprint(\"All configurations match SegFormer for fair comparison.\")","metadata":{},"outputs":[],"execution_count":null}]}