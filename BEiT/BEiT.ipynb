{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12844889,"sourceType":"datasetVersion","datasetId":8124093}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f413843b","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\nimport seaborn as sns\nimport math\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"72c4e04a","cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, dropout_rate=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.dropout(x)\n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, embed_dim=768, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        hidden_dim = int(embed_dim * mlp_ratio)\n        \n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, dropout_rate=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = MLP(embed_dim, mlp_ratio, dropout_rate)\n        \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch, dropout_rate=0.1, use_dropout=True):\n        super().__init__()\n        layers = [\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        ]\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n        \n        layers.extend([\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        ])\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n        \n        self.double_conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, channels, dropout_rate=0.1, use_dropout=True):\n        super().__init__()\n        layers = [\n            nn.Conv2d(channels * 2, channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n        ]\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n            \n        self.fusion_conv = nn.Sequential(*layers)\n        \n    def forward(self, f1, f2):\n        fused = torch.cat([f1, f2], dim=1)\n        return self.fusion_conv(fused)\n\nclass BEiT(nn.Module):\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, out_channels=1, \n                 embed_dim=512, num_heads=8, num_layers=6, mlp_ratio=4, \n                 features=[32, 64, 128, 256], dropout_rate=0.15):\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(dropout_rate)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n        self.fusion1 = FeatureFusion(features[0], dropout_rate, use_dropout=False)\n        self.fusion2 = FeatureFusion(features[1], dropout_rate, use_dropout=False)\n        self.fusion3 = FeatureFusion(features[2], dropout_rate, use_dropout=False)\n        self.fusion4 = FeatureFusion(features[3], dropout_rate=0.1, use_dropout=True)\n        \n        patch_h = patch_w = img_size // patch_size\n        \n        self.decode_proj = nn.Linear(embed_dim, features[3] * 2)\n        \n        self.up4 = nn.ConvTranspose2d(features[3] * 2, features[3], kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(features[3] * 2, features[3], dropout_rate, use_dropout=True)\n        \n        self.up3 = nn.ConvTranspose2d(features[3], features[2], kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(features[2] * 2, features[2], dropout_rate, use_dropout=True)\n        \n        self.up2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(features[1] * 2, features[1], dropout_rate, use_dropout=True)\n        \n        self.up1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(features[0] * 2, features[0], dropout_rate, use_dropout=True)\n        \n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n        \n        self.enc1 = DoubleConv(in_channels, features[0], dropout_rate, use_dropout=False)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(features[0], features[1], dropout_rate, use_dropout=False)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(features[1], features[2], dropout_rate, use_dropout=False)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(features[2], features[3], dropout_rate, use_dropout=False)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n    \n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        for blk in self.blocks:\n            x = blk(x)\n        \n        x = self.norm(x)\n        return x[:, 1:]\n    \n    def forward(self, x):\n        pre_img = x[:, :3, :, :]\n        post_img = x[:, 3:, :, :]\n        \n        pre_e1 = self.enc1(pre_img)\n        pre_p1 = self.pool1(pre_e1)\n        \n        pre_e2 = self.enc2(pre_p1)\n        pre_p2 = self.pool2(pre_e2)\n        \n        pre_e3 = self.enc3(pre_p2)\n        pre_p3 = self.pool3(pre_e3)\n        \n        pre_e4 = self.enc4(pre_p3)\n        \n        post_e1 = self.enc1(post_img)\n        post_p1 = self.pool1(post_e1)\n        \n        post_e2 = self.enc2(post_p1)\n        post_p2 = self.pool2(post_e2)\n        \n        post_e3 = self.enc3(post_p2)\n        post_p3 = self.pool3(post_e3)\n        \n        post_e4 = self.enc4(post_p3)\n        \n        fused_e1 = self.fusion1(pre_e1, post_e1)\n        fused_e2 = self.fusion2(pre_e2, post_e2)\n        fused_e3 = self.fusion3(pre_e3, post_e3)\n        fused_e4 = self.fusion4(pre_e4, post_e4)\n        \n        pre_transformer_features = self.forward_features(pre_img)\n        post_transformer_features = self.forward_features(post_img)\n        \n        combined_features = torch.cat([pre_transformer_features, post_transformer_features], dim=-1)\n        \n        B, N, C = combined_features.shape\n        patch_h = patch_w = int(N ** 0.5)\n        \n        combined_features = self.decode_proj(combined_features)\n        combined_features = combined_features.transpose(1, 2).reshape(B, -1, patch_h, patch_w)\n        \n        d4 = self.up4(combined_features)\n        d4 = torch.cat([d4, fused_e4], dim=1)\n        d4 = self.dec4(d4)\n        \n        d3 = self.up3(d4)\n        d3 = torch.cat([d3, fused_e3], dim=1)\n        d3 = self.dec3(d3)\n        \n        d2 = self.up2(d3)\n        d2 = torch.cat([d2, fused_e2], dim=1)\n        d2 = self.dec2(d2)\n        \n        d1 = self.up1(d2)\n        d1 = torch.cat([d1, fused_e1], dim=1)\n        d1 = self.dec1(d1)\n        \n        out = self.final_conv(d1)\n        return out","metadata":{},"outputs":[],"execution_count":null},{"id":"ced6d14c","cell_type":"code","source":"class EarthquakeDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, img_size=(256, 256)):\n        self.root_dir = root_dir\n        self.split = split\n        self.transform = transform\n        self.img_size = img_size\n        \n        if split == 'train':\n            self.pre_dir = os.path.join(root_dir, 'train', 'A_train_aug')\n            self.post_dir = os.path.join(root_dir, 'train', 'B_train_aug')\n            self.label_dir = os.path.join(root_dir, 'train', 'label_train_aug')\n        elif split == 'val':\n            self.pre_dir = os.path.join(root_dir, 'val', 'A_val')\n            self.post_dir = os.path.join(root_dir, 'val', 'B_val')\n            self.label_dir = os.path.join(root_dir, 'val', 'label_val')\n        else:\n            self.pre_dir = os.path.join(root_dir, 'test', 'A_test')\n            self.post_dir = os.path.join(root_dir, 'test', 'B_test')\n            self.label_dir = os.path.join(root_dir, 'test', 'label_test')\n        \n        self.image_files = sorted([f for f in os.listdir(self.pre_dir) if f.endswith('.png')])\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        \n        pre_img = Image.open(os.path.join(self.pre_dir, img_name)).convert('RGB')\n        post_img = Image.open(os.path.join(self.post_dir, img_name)).convert('RGB')\n        label = Image.open(os.path.join(self.label_dir, img_name)).convert('L')\n        \n        pre_img = pre_img.resize(self.img_size)\n        post_img = post_img.resize(self.img_size)\n        label = label.resize(self.img_size)\n        \n        if self.transform:\n            pre_img = self.transform(pre_img)\n            post_img = self.transform(post_img)\n        else:\n            pre_img = transforms.ToTensor()(pre_img)\n            post_img = transforms.ToTensor()(post_img)\n        \n        label = transforms.ToTensor()(label)\n        \n        label = (label > 0.5).float()\n        \n        combined_img = torch.cat([pre_img, post_img], dim=0)\n        \n        return combined_img, label\n\nclass EarlyStopping:\n    def __init__(self, patience=15, min_delta=1e-4, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()","metadata":{},"outputs":[],"execution_count":null},{"id":"db709c3d","cell_type":"code","source":"def dice_coef(pred, target, smooth=1e-6):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    dice = (2. * intersection + smooth) / (denom + smooth)\n    return dice.mean()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        dice = dice_coef(pred, target, smooth=self.smooth)\n        return 1.0 - dice\n\ndef combined_loss(logits, mask, bce_weight=0.6, dice_weight=0.4):\n    bce_loss = nn.BCEWithLogitsLoss()\n    bce = bce_loss(logits, mask)\n    probs = torch.sigmoid(logits)\n    dice = DiceLoss()(probs, mask)\n    return bce_weight * bce + dice_weight * dice\n\n@torch.no_grad()\ndef compute_metrics_batch(logits, masks, thresh=0.5):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= thresh).float()\n    preds_flat = preds.view(-1).cpu().numpy()\n    masks_flat = masks.view(-1).cpu().numpy()\n    \n    unique_preds = np.unique(preds_flat)\n    unique_masks = np.unique(masks_flat)\n    \n    if len(unique_preds) == 1 and len(unique_masks) == 1:\n        if unique_preds[0] == unique_masks[0]:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = len(preds_flat), 0, 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, 0, len(preds_flat)\n        else:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = 0, len(preds_flat), 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, len(preds_flat), 0\n    else:\n        cm = confusion_matrix(masks_flat, preds_flat, labels=[0,1])\n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n        else:\n            if cm.shape == (1, 1):\n                if unique_masks[0] == 0:\n                    tn, fp, fn, tp = cm[0,0], 0, 0, 0\n                else:\n                    tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n            else:\n                tn, fp, fn, tp = 0, 0, 0, 0\n    \n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    iou_background = tn / (tn + fp + fn + eps)\n    iou_damage = tp / (tp + fp + fn + eps)\n    miou = (iou_background + iou_damage) / 2\n    \n    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n            \"iou\": float(iou), \"dice\": float(dice), \"precision\": float(precision),\n            \"recall\": float(recall), \"f1\": float(f1), \"acc\": float(acc), \"miou\": float(miou)}","metadata":{},"outputs":[],"execution_count":null},{"id":"fd5c7a47","cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, scheduler=None):\n    model.train()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    \n    for imgs, masks in tqdm(loader, desc=\"Train batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n        \n        metas = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += metas[k]\n    \n    if scheduler is not None:\n        scheduler.step()\n    \n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    train_acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    return epoch_loss, train_acc\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    \n    for imgs, masks in tqdm(loader, desc=\"Val batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        running_loss += loss.item() * imgs.size(0)\n        metas = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += metas[k]\n    \n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    iou_background = tn / (tn + fp + fn + eps)\n    iou_damage = tp / (tp + fp + fn + eps)\n    miou = (iou_background + iou_damage) / 2\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    metrics = {\"loss\": epoch_loss, \"iou\": iou, \"dice\": dice, \"precision\": precision,\n               \"recall\": recall, \"f1\": f1, \"acc\": acc, \"miou\": miou,\n               \"confusion\": np.array([[tn, fp], [fn, tp]])}\n    return metrics\n\ndef train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, patience=15):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        if p.ndim == 1 or name.endswith(\".bias\"):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    optimizer = optim.AdamW(\n        [\n            {\"params\": decay, \"weight_decay\": 1e-3},\n            {\"params\": no_decay, \"weight_decay\": 0.0},\n        ],\n        lr=learning_rate,\n    )\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.7, patience=7, verbose=True, min_lr=1e-6\n    )\n    \n    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n    \n    train_losses = []\n    val_losses = []\n    val_ious = []\n    val_dices = []\n    train_accs = []\n    val_accs = []\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n        \n        val_metrics = validate(model, val_loader)\n        \n        scheduler.step(val_metrics['loss'])\n        \n        print(f\"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_metrics['loss']:.6f} | Val Acc: {val_metrics['acc']:.4f} | IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | F1: {val_metrics['f1']:.4f}\")\n        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_metrics['loss'])\n        val_ious.append(val_metrics['iou'])\n        val_dices.append(val_metrics['dice'])\n        train_accs.append(train_acc)\n        val_accs.append(val_metrics['acc'])\n        \n        if val_metrics['loss'] < best_val_loss - 1e-4:\n            best_val_loss = val_metrics['loss']\n            torch.save(model.state_dict(), 'best_beit_earthquake.pth')\n            print(f\"Saved best model with validation loss: {best_val_loss:.6f}\")\n        \n        if early_stopping(val_metrics['loss'], model):\n            print(f'Early stopping triggered after {epoch+1} epochs')\n            break\n    \n    return train_losses, val_losses, val_ious, val_dices, train_accs, val_accs","metadata":{},"outputs":[],"execution_count":null},{"id":"13dd4e90","cell_type":"code","source":"def main():\n    BATCH_SIZE = 6\n    LEARNING_RATE = 2e-4\n    NUM_EPOCHS = 200\n    PATIENCE = 10\n    IMG_SIZE = (256, 256)\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    root_dir = '/kaggle/input/finaldataset/earthquakeDataset'\n    \n    train_dataset = EarthquakeDataset(root_dir, split='train', transform=transform, img_size=IMG_SIZE)\n    val_dataset = EarthquakeDataset(root_dir, split='val', transform=transform, img_size=IMG_SIZE)\n    test_dataset = EarthquakeDataset(root_dir, split='test', transform=transform, img_size=IMG_SIZE)\n    \n    train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n    \n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    print(f\"Test samples: {len(test_dataset)}\")\n    \n    model = BEiT(img_size=256, patch_size=16, in_channels=3, out_channels=1, \n                embed_dim=512, num_heads=8, num_layers=6, mlp_ratio=4, \n                features=[32, 64, 128, 256], dropout_rate=0.15).to(device)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")\n    \n    print(\"Starting BEiT training...\")\n    train_losses, val_losses, val_ious, val_dices, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE, PATIENCE\n    )\n    \n    return model, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader\n\nmodel, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader = main()","metadata":{},"outputs":[],"execution_count":null},{"id":"7dc292ba","cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\nax1.plot(train_losses, label='Training Loss')\nax1.plot(val_losses, label='Validation Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_ylim(0, 1)\nax1.legend()\nax1.set_title('BEiT Training and Validation Loss')\n\nax2.plot(train_accs, label='Training Accuracy', color='blue')\nax2.plot(val_accs, label='Validation Accuracy', color='red')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.set_ylim(0, 1)\nax2.legend()\nax2.set_title('BEiT Training and Validation Accuracy')\n\nax3.plot(val_ious, label='Validation IoU', color='green')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('IoU')\nax3.set_ylim(0, 1)\nax3.legend()\nax3.set_title('BEiT Validation IoU')\n\nax4.plot(val_dices, label='Validation Dice', color='orange')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Dice Coefficient')\nax4.set_ylim(0, 1)\nax4.legend()\nax4.set_title('BEiT Validation Dice Coefficient')\n\nplt.tight_layout()\nplt.savefig('beit_training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"db37837a","cell_type":"code","source":"print(\"Loading best BEiT model for test evaluation...\")\nmodel.load_state_dict(torch.load('best_beit_earthquake.pth', map_location=device))\nmodel.to(device)\n\nprint(\"\\nEvaluating BEiT on test set...\")\ntest_metrics = validate(model, test_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"BEiT (BIDIRECTIONAL ENCODER REPRESENTATION FROM IMAGE TRANSFORMERS) TEST EVALUATION METRICS\")\nprint(\"=\"*50)\nprint(f\"Test set processed with batch_size=1\")\nprint(f\"Loss:            {test_metrics['loss']:.6f}\")\nprint(f\"IoU:             {test_metrics['iou']:.4f}\")\nprint(f\"mIoU:            {test_metrics['miou']:.4f}\")\nprint(f\"Dice Coefficient: {test_metrics['dice']:.4f}\")\nprint(f\"Accuracy:        {test_metrics['acc']:.4f}\")\nprint(f\"Precision:       {test_metrics['precision']:.4f}\")\nprint(f\"Recall:          {test_metrics['recall']:.4f}\")\nprint(f\"F1-Score:        {test_metrics['f1']:.4f}\")\nprint(\"Confusion matrix (pixel-level):\")\nprint(test_metrics[\"confusion\"])\nprint(\"=\"*50)\n\nplt.figure(figsize=(8, 6))\ncm = test_metrics[\"confusion\"]\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Predicted No Damage', 'Predicted Damage'],\n            yticklabels=['Actual No Damage', 'Actual Damage'])\nplt.title('BEiT Test Set Confusion Matrix (Pixel-level)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.text(0.5, -0.1, f'TN: {cm[0,0]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(1.5, -0.1, f'FP: {cm[0,1]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(0.5, -0.15, f'FN: {cm[1,0]:,}', ha='center', transform=plt.gca().transAxes)\nplt.text(1.5, -0.15, f'TP: {cm[1,1]:,}', ha='center', transform=plt.gca().transAxes)\n\nplt.tight_layout()\nplt.savefig('beit_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"BEiT training and evaluation completed!\")","metadata":{},"outputs":[],"execution_count":null}]}