{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12844889,"sourceType":"datasetVersion","datasetId":8124093}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2af3fe5b","cell_type":"markdown","source":"# Siamese U-Net with Coordinate Attention for Earthquake Damage Assessment\n\n\n","metadata":{}},{"id":"fbb91ce4","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\nimport seaborn as sns\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch, dropout_rate=0.1, use_dropout=True):\n        super().__init__()\n        layers = [\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        ]\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n        \n        layers.extend([\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        ])\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n        \n        self.double_conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass CoordinateAttention(nn.Module):\n\n    def __init__(self, inp, reduction=32):\n        super(CoordinateAttention, self).__init__()\n        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n\n        mip = max(8, inp // reduction)\n\n        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n        self.bn1 = nn.BatchNorm2d(mip)\n        self.act = nn.ReLU()\n\n        self.conv_h = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n        self.conv_w = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        identity = x\n        n, c, h, w = x.size()\n        \n        x_h = self.pool_h(x)\n        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n\n        y = torch.cat([x_h, x_w], dim=2)\n        \n        y = self.conv1(y)\n        y = self.bn1(y)\n        y = self.act(y)\n\n        x_h, x_w = torch.split(y, [h, w], dim=2)\n        x_w = x_w.permute(0, 1, 3, 2)\n\n        a_h = self.conv_h(x_h).sigmoid()\n        a_w = self.conv_w(x_w).sigmoid()\n\n        out = identity * a_w * a_h\n\n        return out\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, channels, dropout_rate=0.1, use_dropout=True):\n        super().__init__()\n        layers = [\n            nn.Conv2d(channels * 2, channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n        ]\n        if use_dropout:\n            layers.append(nn.Dropout2d(dropout_rate))\n        self.fusion_conv = nn.Sequential(*layers)\n        \n        self.coordinate_attention = CoordinateAttention(channels)\n\n    def forward(self, f1, f2):\n        fused = torch.cat([f1, f2], dim=1)\n        fused = self.fusion_conv(fused)\n        fused = self.coordinate_attention(fused)\n        return fused\n\nclass SiameseEncoder(nn.Module):\n    def __init__(self, in_channels=3, features=[64,128,256,512], dropout_rate=0.1):\n        super().__init__()\n        self.enc1 = DoubleConv(in_channels, features[0], dropout_rate, use_dropout=False)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(features[0], features[1], dropout_rate, use_dropout=False)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(features[1], features[2], dropout_rate, use_dropout=False)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(features[2], features[3], dropout_rate = 0.15, use_dropout=False)\n        self.pool4 = nn.MaxPool2d(2)\n        self.bottleneck = DoubleConv(features[3], features[3] * 2, dropout_rate = 0.2, use_dropout=True)\n\n    def forward(self, x):\n        e1 = self.enc1(x); p1 = self.pool1(e1)\n        e2 = self.enc2(p1); p2 = self.pool2(e2)\n        e3 = self.enc3(p2); p3 = self.pool3(e3)\n        e4 = self.enc4(p3); p4 = self.pool4(e4)\n        bottleneck = self.bottleneck(p4)\n        return [e1, e2, e3, e4, bottleneck]\n\nclass SiameseUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[32,64,128,256], dropout_rate=0.15):\n        super().__init__()\n        self.encoder = SiameseEncoder(in_channels, features, dropout_rate)\n        self.fusion1 = FeatureFusion(features[0], dropout_rate, use_dropout=False)\n        self.fusion2 = FeatureFusion(features[1], dropout_rate, use_dropout=False)\n        self.fusion3 = FeatureFusion(features[2], dropout_rate, use_dropout=False)\n        self.fusion4 = FeatureFusion(features[3], dropout_rate=0.1, use_dropout=True)\n        self.fusion_bottleneck = FeatureFusion(features[3] * 2, dropout_rate=0.5, use_dropout=True)\n\n        self.up4 = nn.ConvTranspose2d(features[3] * 2, features[3], kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(features[3] * 2, features[3], dropout_rate, use_dropout=True)\n\n        self.up3 = nn.ConvTranspose2d(features[3], features[2], kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(features[2] * 2, features[2], dropout_rate, use_dropout=True)\n\n        self.up2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(features[1] * 2, features[1], dropout_rate, use_dropout=True)\n\n        self.up1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(features[0] * 2, features[0], dropout_rate, use_dropout=True)\n\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        pre_img = x[:, :3, :, :]\n        post_img = x[:, 3:, :, :]\n        pre_features = self.encoder(pre_img)\n        post_features = self.encoder(post_img)\n        fused_e1 = self.fusion1(pre_features[0], post_features[0])\n        fused_e2 = self.fusion2(pre_features[1], post_features[1])\n        fused_e3 = self.fusion3(pre_features[2], post_features[2])\n        fused_e4 = self.fusion4(pre_features[3], post_features[3])\n        fused_bottleneck = self.fusion_bottleneck(pre_features[4], post_features[4])\n\n        d4 = self.up4(fused_bottleneck)\n        d4 = torch.cat([d4, fused_e4], dim=1)\n        d4 = self.dec4(d4)\n\n        d3 = self.up3(d4)\n        d3 = torch.cat([d3, fused_e3], dim=1)\n        d3 = self.dec3(d3)\n\n        d2 = self.up2(d3)\n        d2 = torch.cat([d2, fused_e2], dim=1)\n        d2 = self.dec2(d2)\n\n        d1 = self.up1(d2)\n        d1 = torch.cat([d1, fused_e1], dim=1)\n        d1 = self.dec1(d1)\n\n        out = self.final_conv(d1)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:58:47.105702Z","iopub.execute_input":"2025-11-12T15:58:47.106192Z","iopub.status.idle":"2025-11-12T15:58:59.913577Z","shell.execute_reply.started":"2025-11-12T15:58:47.106143Z","shell.execute_reply":"2025-11-12T15:58:59.912304Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":1},{"id":"45d3d3a6","cell_type":"code","source":"class EarthquakeDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, img_size=(256, 256)):\n        self.root_dir = root_dir\n        self.split = split\n        self.transform = transform\n        self.img_size = img_size\n        \n        if split == 'train':\n            self.pre_dir = os.path.join(root_dir, 'train', 'A_train_aug')\n            self.post_dir = os.path.join(root_dir, 'train', 'B_train_aug')\n            self.label_dir = os.path.join(root_dir, 'train', 'label_train_aug')\n        elif split == 'val':\n            self.pre_dir = os.path.join(root_dir, 'val', 'A_val')\n            self.post_dir = os.path.join(root_dir, 'val', 'B_val')\n            self.label_dir = os.path.join(root_dir, 'val', 'label_val')\n        else:\n            self.pre_dir = os.path.join(root_dir, 'test', 'A_test')\n            self.post_dir = os.path.join(root_dir, 'test', 'B_test')\n            self.label_dir = os.path.join(root_dir, 'test', 'label_test')\n        \n        self.image_files = sorted([f for f in os.listdir(self.pre_dir) if f.endswith('.png')])\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        \n        pre_img = Image.open(os.path.join(self.pre_dir, img_name)).convert('RGB')\n        post_img = Image.open(os.path.join(self.post_dir, img_name)).convert('RGB')\n        label = Image.open(os.path.join(self.label_dir, img_name)).convert('L')\n        \n        pre_img = pre_img.resize(self.img_size)\n        post_img = post_img.resize(self.img_size)\n        label = label.resize(self.img_size)\n        \n        if self.transform:\n            pre_img = self.transform(pre_img)\n            post_img = self.transform(post_img)\n        else:\n            pre_img = transforms.ToTensor()(pre_img)\n            post_img = transforms.ToTensor()(post_img)\n        \n        label = transforms.ToTensor()(label)\n        label = (label > 0.5).float()\n        combined_img = torch.cat([pre_img, post_img], dim=0)\n        \n        return combined_img, label\n\nclass EarlyStopping:\n    def __init__(self, patience=15, min_delta=1e-4, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\ndef dice_coef(pred, target, smooth=1e-6):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    dice = (2. * intersection + smooth) / (denom + smooth)\n    return dice.mean()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        dice = dice_coef(pred, target, smooth=self.smooth)\n        return 1.0 - dice\n\ndef combined_loss(logits, mask, bce_weight=0.6, dice_weight=0.4):\n    bce_loss = nn.BCEWithLogitsLoss()\n    bce = bce_loss(logits, mask)\n    probs = torch.sigmoid(logits)\n    dice = DiceLoss()(probs, mask)\n    return bce_weight * bce + dice_weight * dice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:58:59.915049Z","iopub.execute_input":"2025-11-12T15:58:59.915575Z","iopub.status.idle":"2025-11-12T15:58:59.933386Z","shell.execute_reply.started":"2025-11-12T15:58:59.915549Z","shell.execute_reply":"2025-11-12T15:58:59.932529Z"}},"outputs":[],"execution_count":2},{"id":"8631ac61","cell_type":"code","source":"@torch.no_grad()\ndef compute_metrics_batch(logits, masks, thresh=0.5):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= thresh).float()\n    preds_flat = preds.view(-1).cpu().numpy()\n    masks_flat = masks.view(-1).cpu().numpy()\n    \n    unique_preds = np.unique(preds_flat)\n    unique_masks = np.unique(masks_flat)\n    \n    if len(unique_preds) == 1 and len(unique_masks) == 1:\n        if unique_preds[0] == unique_masks[0]:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = len(preds_flat), 0, 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, 0, len(preds_flat)\n        else:\n            if unique_preds[0] == 1:\n                tp, fp, fn, tn = 0, len(preds_flat), 0, 0\n            else:\n                tp, fp, fn, tn = 0, 0, len(preds_flat), 0\n    else:\n        cm = confusion_matrix(masks_flat, preds_flat, labels=[0,1])\n        if cm.shape == (2, 2):\n            tn, fp, fn, tp = cm.ravel()\n        else:\n            if cm.shape == (1, 1):\n                if unique_masks[0] == 0:\n                    tn, fp, fn, tp = cm[0,0], 0, 0, 0\n                else:\n                    tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n            else:\n                tn, fp, fn, tp = 0, 0, 0, 0\n    \n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    iou_bg = tn / (tn + fp + fn + eps)\n    miou = (iou + iou_bg) / 2\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n            \"iou\": float(iou), \"miou\": float(miou), \"dice\": float(dice), \"precision\": float(precision),\n            \"recall\": float(recall), \"f1\": float(f1), \"acc\": float(acc)}\n\ndef train_one_epoch(model, loader, optimizer, scheduler=None):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for imgs, masks in tqdm(loader, desc=\"Train batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n        \n        metrics = compute_metrics_batch(logits, masks)\n        running_acc += metrics['acc'] * imgs.size(0)\n    \n    if scheduler is not None:\n        scheduler.step()\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    \n    for imgs, masks in tqdm(loader, desc=\"Val batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        running_loss += loss.item() * imgs.size(0)\n        metas = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += metas[k]\n    \n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    iou_bg = tn / (tn + fp + fn + eps)\n    miou = (iou + iou_bg) / 2\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    \n    epoch_loss = running_loss / len(loader.dataset)\n    metrics = {\"loss\": epoch_loss, \"iou\": iou, \"miou\": miou, \"dice\": dice, \"precision\": precision,\n               \"recall\": recall, \"f1\": f1, \"acc\": acc, \n               \"confusion\": np.array([[tn, fp], [fn, tp]])}\n    return metrics\n\ndef plot_confusion_matrix(confusion_matrix, title='Confusion Matrix'):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', \n                xticklabels=['Non-damage', 'Damage'], \n                yticklabels=['Non-damage', 'Damage'])\n    plt.title(title)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:58:59.934561Z","iopub.execute_input":"2025-11-12T15:58:59.935282Z","iopub.status.idle":"2025-11-12T15:58:59.963031Z","shell.execute_reply.started":"2025-11-12T15:58:59.935251Z","shell.execute_reply":"2025-11-12T15:58:59.962036Z"}},"outputs":[],"execution_count":3},{"id":"56a3a2f9","cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=200, learning_rate=2e-4, patience=10):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n        if p.ndim == 1 or name.endswith(\".bias\"):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    optimizer = optim.AdamW(\n        [\n            {\"params\": decay, \"weight_decay\": 1e-4},\n            {\"params\": no_decay, \"weight_decay\": 0.0},\n        ],\n        lr=learning_rate,\n    )\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.7, patience=7, verbose=True, min_lr=1e-6\n    )\n    \n    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n    \n    train_losses = []\n    train_accs = []\n    val_losses = []\n    val_accs = []\n    val_ious = []\n    val_dices = []\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        \n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n        val_metrics = validate(model, val_loader)\n        \n        scheduler.step(val_metrics['loss'])\n        \n        print(f\"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_metrics['loss']:.6f} | Val Acc: {val_metrics['acc']:.4f} | IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | F1: {val_metrics['f1']:.4f}\")\n        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        val_losses.append(val_metrics['loss'])\n        val_accs.append(val_metrics['acc'])\n        val_ious.append(val_metrics['iou'])\n        val_dices.append(val_metrics['dice'])\n        \n        if val_metrics['loss'] < best_val_loss - 1e-4:\n            best_val_loss = val_metrics['loss']\n            torch.save(model.state_dict(), 'best_siamese_unet_coordinate_attention.pth')\n            print(f\"Saved best model with validation loss: {best_val_loss:.6f}\")\n        \n        if early_stopping(val_metrics['loss'], model):\n            print(f'Early stopping triggered after {epoch+1} epochs')\n            break\n    \n    return train_losses, train_accs, val_losses, val_accs, val_ious, val_dices\n\ndef main():\n    BATCH_SIZE = 6\n    LEARNING_RATE = 2e-4\n    NUM_EPOCHS = 200\n    PATIENCE = 10\n    IMG_SIZE = (256, 256)\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    root_dir = '/kaggle/input/finaldatasetnew/earthquakedatasetnew'\n    \n    train_dataset = EarthquakeDataset(root_dir, split='train', transform=transform, img_size=IMG_SIZE)\n    val_dataset = EarthquakeDataset(root_dir, split='val', transform=transform, img_size=IMG_SIZE)\n    test_dataset = EarthquakeDataset(root_dir, split='test', transform=transform, img_size=IMG_SIZE)\n    \n    train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n    \n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    print(f\"Test samples: {len(test_dataset)}\")\n    \n    model = SiameseUNet(in_channels=3, out_channels=1, features=[32, 64, 128, 256], dropout_rate=0.15).to(device)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")\n    print(\"MODEL: Siamese U-Net with COORDINATE ATTENTION at fusion\")\n    \n    print(\"Starting training...\")\n    train_losses, train_accs, val_losses, val_accs, val_ious, val_dices = train_model(\n        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE, PATIENCE\n    )\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    ax1.plot(train_losses, label='Training Loss')\n    ax1.plot(val_losses, label='Validation Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_ylim(0, 1)\n    ax1.legend()\n    ax1.set_title('Training and Validation Loss (Coordinate Attention)')\n    \n    ax2.plot(train_accs, label='Training Accuracy')\n    ax2.plot(val_accs, label='Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_ylim(0, 1)\n    ax2.legend()\n    ax2.set_title('Training and Validation Accuracy (Coordinate Attention)')\n    \n    ax3.plot(val_ious, label='Validation IoU', color='green')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('IoU')\n    ax3.set_ylim(0, 1)\n    ax3.legend()\n    ax3.set_title('Validation IoU (Coordinate Attention)')\n    \n    ax4.plot(val_ious, label='IoU', color='green')\n    ax4.plot(val_dices, label='Dice', color='orange')\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Score')\n    ax4.set_ylim(0, 1)\n    ax4.legend()\n    ax4.set_title('Validation Metrics (Coordinate Attention)')\n    \n    plt.tight_layout()\n    plt.savefig('training_curves_coordinate_attention.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"Loading best model for test evaluation...\")\n    model.load_state_dict(torch.load('best_siamese_unet_coordinate_attention.pth', map_location=device))\n    model.to(device)\n    \n    print(\"\\nEvaluating on test set...\")\n    test_metrics = validate(model, test_loader)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"SIAMESE U-NET WITH COORDINATE ATTENTION TEST EVALUATION\")\n    print(\"=\"*50)\n    print(f\"Test set processed with batch_size=1\")\n    print(f\"Loss:            {test_metrics['loss']:.6f}\")\n    print(f\"IoU:             {test_metrics['iou']:.4f}\")\n    print(f\"mIoU:            {test_metrics['miou']:.4f}\")\n    print(f\"Dice Coefficient: {test_metrics['dice']:.4f}\")\n    print(f\"Accuracy:        {test_metrics['acc']:.4f}\")\n    print(f\"Precision:       {test_metrics['precision']:.4f}\")\n    print(f\"Recall:          {test_metrics['recall']:.4f}\")\n    print(f\"F1-Score:        {test_metrics['f1']:.4f}\")\n    print(\"Confusion matrix (pixel-level):\")\n    print(test_metrics[\"confusion\"])\n    print(\"=\"*50)\n    \n    plot_confusion_matrix(test_metrics[\"confusion\"], \"Test Set Confusion Matrix (Coordinate Attention)\")\n    \n    print(\"Siamese U-Net (Coordinate Attention) training and evaluation completed!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:58:59.964761Z","iopub.execute_input":"2025-11-12T15:58:59.965042Z","execution_failed":"2025-11-12T15:59:11.466Z"}},"outputs":[{"name":"stdout","text":"Training samples: 3972\nValidation samples: 331\nTest samples: 332\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Total parameters: 14,088,561\nMODEL: Siamese U-Net with COORDINATE ATTENTION at fusion\nStarting training...\n\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"Train batch:   0%|          | 0/662 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null}]}